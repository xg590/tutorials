{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T12:25:32.540037Z",
     "start_time": "2022-07-15T12:25:32.537387Z"
    }
   },
   "outputs": [],
   "source": [
    "# Rewrite of https://github.com/harvardnlp/annotated-transformer/blob/master/The%20Annotated%20Transformer.ipynb\n",
    "# pip install torch matplotlib spacy torchtext seaborn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T12:25:32.932556Z",
     "start_time": "2022-07-15T12:25:32.541646Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch, copy, pdb, math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T12:25:32.937663Z",
     "start_time": "2022-07-15T12:25:32.935307Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 1 # number of encoder or decoder stacks \n",
    "h = 8 # heards in parallel\n",
    "d_model = 512 # dimensions \n",
    "dropout_rate = 0.1\n",
    "d_ff = 2048\n",
    "\n",
    "vocab_size_src  = 51\n",
    "vocab_size_tgt  = 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T12:25:32.957921Z",
     "start_time": "2022-07-15T12:25:32.939350Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(Embedding, self).__init__() \n",
    "        self.emb = nn.Embedding(vocab_size, d_model) # https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x) * math.sqrt(d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T12:25:32.970089Z",
     "start_time": "2022-07-15T12:25:32.959310Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def PositionalEncoding(): \n",
    "    # Implement the PE function.\n",
    "    # Compute the positional encodings once in log space.\n",
    "    max_len = 999\n",
    "    position = torch.arange(0, max_len).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "    pe = torch.zeros(max_len, d_model, requires_grad=False)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe.unsqueeze(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T12:25:32.982658Z",
     "start_time": "2022-07-15T12:25:32.971386Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k     = d_model // h\n",
    "        self.h       = h\n",
    "        self.l1      = nn.Linear(d_model, d_model) \n",
    "        self.l2      = nn.Linear(d_model, d_model) \n",
    "        self.l3      = nn.Linear(d_model, d_model) \n",
    "        self.l4      = nn.Linear(d_model, d_model)  \n",
    "        \n",
    "    def attention(self, Q, K, V, mask=None):\n",
    "        \"Compute 'Scaled Dot Product Attention'\"\n",
    "        d_k = Q.size(-1)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9) \n",
    "        weights = F.softmax(scores, dim = -1) \n",
    "        x = torch.matmul(weights, V) \n",
    "        if mask is not None:\n",
    "            print('Q:', Q.shape)\n",
    "            print('x:', x.shape)\n",
    "            print('mask:', mask.shape) \n",
    "            print('scores:', scores.shape) \n",
    "        return x\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        if mask is not None:\n",
    "            print('pre:',Q)\n",
    "            print('pre==decoder_post_eb:',Q.shape)\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = Q.size(0) \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        Q = self.l1(Q).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        K = self.l2(K).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        V = self.l3(V).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "         \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        #print('post:',Q)\n",
    "        x = self.attention(Q, K, V, mask=mask) \n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, d_model)\n",
    "        x = self.l4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T12:25:32.994750Z",
     "start_time": "2022-07-15T12:25:32.983918Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.l1 = nn.Linear(d_model, d_ff)\n",
    "        self.l2 = nn.Linear(d_ff, d_model) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = F.relu(x) \n",
    "        x = self.l2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T12:25:33.003981Z",
     "start_time": "2022-07-15T12:25:32.996127Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadedAttention()\n",
    "        self.ff = FeedForward( ) \n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=1e-6) \n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    " \n",
    "    def forward(self, x):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = x + self.dropout1(self.mha(self.norm1(x), self.norm1(x), self.norm1(x)))  \n",
    "        x = x + self.dropout2(self.ff (self.norm2(x)))  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T12:25:33.012053Z",
     "start_time": "2022-07-15T12:25:33.005360Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
    "    def __init__(self):\n",
    "        super(DecoderLayer, self).__init__() \n",
    "        self.mmha     = MultiHeadedAttention()\n",
    "        self.mha      = MultiHeadedAttention() \n",
    "        self.ff       = FeedForward( ) \n",
    "        self.norm1    = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm2    = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm3    = nn.LayerNorm(d_model, eps=1e-6) \n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)  \n",
    " \n",
    "    def forward(self, x, memory, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\" \n",
    "        x = x + self.dropout1(self.mmha(x, x, x, tgt_mask) )   \n",
    "        x = x + self.dropout2(self.mha (x, memory, memory))    \n",
    "        x = x + self.dropout3(self.ff  (x))    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T12:25:33.025701Z",
     "start_time": "2022-07-15T12:25:33.014423Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T12:25:33.034626Z",
     "start_time": "2022-07-15T12:25:33.027101Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many \n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "          \n",
    "        self.src_embed     = Embedding(vocab_size_src)\n",
    "        self.pe            = PositionalEncoding()\n",
    "        self.encoderLayers = nn.ModuleList([copy.deepcopy(EncoderLayer()) for _ in range(N)]) \n",
    "        self.norm1         = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        \n",
    "        self.tgt_embed     = Embedding(vocab_size_tgt)\n",
    "        self.decoderLayers = nn.ModuleList([copy.deepcopy(DecoderLayer()) for _ in range(N)])  \n",
    "        self.generator     = Generator(d_model, vocab_size_tgt)\n",
    "        self.norm2         = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        \n",
    "    def forward(self, src, tgt, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        memory = self.norm1(self.encode(src))\n",
    "        tgt    = self.norm2(self.decode(memory, tgt, tgt_mask)) \n",
    "        return tgt \n",
    "    \n",
    "    def encode(self, src):\n",
    "        src = self.src_embed(src) \n",
    "        src = src + self.pe[:, :src.size(1)]  \n",
    "        for layer in self.encoderLayers:\n",
    "            src = layer(src) \n",
    "        memory = src\n",
    "        return memory\n",
    "    \n",
    "    def decode(self, memory, tgt, tgt_mask):\n",
    "        print('decoder_pre_eb:', tgt.shape)\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        print('decoder_post_eb:', tgt.shape)\n",
    "        print('decoder_post_eb:', tgt)\n",
    "        for layer in self.decoderLayers: \n",
    "            tgt = layer(tgt, memory, tgt_mask) \n",
    "        return tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T12:25:33.045805Z",
     "start_time": "2022-07-15T12:25:33.036114Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, \n",
    "               d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    model = Transformer(\n",
    "        )\n",
    "    \n",
    "    # This was important from their code. \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T12:25:33.054713Z",
     "start_time": "2022-07-15T12:25:33.047182Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    attn_shape = (1, size, size) \n",
    "    return torch.triu(torch.ones(attn_shape, dtype=int), diagonal=1) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T12:25:33.069176Z",
     "start_time": "2022-07-15T12:25:33.056140Z"
    }
   },
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"Object for holding a batch of data with mask during training.\"\n",
    "    def __init__(self, src, trg=None, pad=0):\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :-1]\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            self.trg_mask = self.make_std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "     \n",
    "    def make_std_mask(self, tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & subsequent_mask(tgt.size(-1)) \n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T12:25:33.076599Z",
     "start_time": "2022-07-15T12:25:33.070408Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_epoch(data_iter, model, loss_compute):\n",
    "    \"Standard Training and Logging Function\" \n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = model.forward(batch.src, batch.trg, batch.trg_mask)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T12:25:33.084789Z",
     "start_time": "2022-07-15T12:25:33.077810Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "global max_src_in_batch, max_tgt_in_batch\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T12:25:33.095044Z",
     "start_time": "2022-07-15T12:25:33.086149Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = d_model\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * (self.model_size ** (-0.5) * min(step ** (-0.5), step * self.warmup ** (-1.5))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T12:25:33.104788Z",
     "start_time": "2022-07-15T12:25:33.097799Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_gen(V, batch, nbatches): \n",
    "    \"Generate random data for a src-tgt copy task.\"\n",
    "    for i in range(nbatches): \n",
    "        src = torch.randint(1, V, size=(batch, V), requires_grad=False)\n",
    "        src[:, 0] = 1\n",
    "        tgt = copy.deepcopy(src)\n",
    "        yield Batch(src, tgt, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T12:25:33.115879Z",
     "start_time": "2022-07-15T12:25:33.105976Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "        \n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        loss  = self.criterion(x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)) \n",
    "        loss /= norm\n",
    "        loss.backward()\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad() \n",
    "        return loss.data * norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T12:25:33.248484Z",
     "start_time": "2022-07-15T12:25:33.117147Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_pre_eb: torch.Size([1, 50])\n",
      "decoder_post_eb: torch.Size([1, 50, 512])\n",
      "decoder_post_eb: tensor([[[-0.7567, -1.2534,  1.8538,  ...,  1.2469, -0.1786, -0.3044],\n",
      "         [-0.6531,  0.7032,  1.2039,  ..., -0.6993, -0.8673,  0.9826],\n",
      "         [-0.8913, -1.2010,  0.5727,  ..., -2.2280,  2.1302, -1.4048],\n",
      "         ...,\n",
      "         [-0.7579,  0.4750,  1.3414,  ...,  2.0982,  1.1911,  1.9675],\n",
      "         [-1.3025, -2.1970, -1.1736,  ..., -0.6874, -0.5123, -1.4236],\n",
      "         [-2.2855,  1.6247, -0.6763,  ...,  1.4621,  0.3762, -1.1457]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "pre: tensor([[[-0.7567, -1.2534,  1.8538,  ...,  1.2469, -0.1786, -0.3044],\n",
      "         [-0.6531,  0.7032,  1.2039,  ..., -0.6993, -0.8673,  0.9826],\n",
      "         [-0.8913, -1.2010,  0.5727,  ..., -2.2280,  2.1302, -1.4048],\n",
      "         ...,\n",
      "         [-0.7579,  0.4750,  1.3414,  ...,  2.0982,  1.1911,  1.9675],\n",
      "         [-1.3025, -2.1970, -1.1736,  ..., -0.6874, -0.5123, -1.4236],\n",
      "         [-2.2855,  1.6247, -0.6763,  ...,  1.4621,  0.3762, -1.1457]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "pre==decoder_post_eb: torch.Size([1, 50, 512])\n",
      "Q: torch.Size([1, 8, 50, 64])\n",
      "x: torch.Size([1, 8, 50, 64])\n",
      "mask: torch.Size([1, 1, 50, 50])\n",
      "scores: torch.Size([1, 8, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "# Train the simple copy task.\n",
    "V = 51\n",
    "criterion = nn.CrossEntropyLoss()#LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
    "\n",
    "model = make_model(V, V)\n",
    "model_opt = NoamOpt(d_model, \n",
    "                    1, \n",
    "                    400, \n",
    "                    torch.optim.Adam(model.parameters(), \n",
    "                                     lr=0, \n",
    "                                     betas=(0.9, 0.98), \n",
    "                                     eps=1e-9\n",
    "                                    )\n",
    "                   )\n",
    "\n",
    "model.train()\n",
    "run_epoch(data_gen(V, 1, 1), model, SimpleLossCompute(model.generator, criterion, model_opt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
