{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9cf52f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-29T22:46:29.830803Z",
     "start_time": "2024-03-29T22:46:29.284777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64]) torch.Size([32])\n",
      "tensor([[ 0.0046],\n",
      "        [-0.0085]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "class Net(nn.Module): \n",
    "    def __init__(self, debug=False):\n",
    "        super(Net, self).__init__() \n",
    "        self.l1 = nn.Linear(in_features=10, out_features=64)\n",
    "        self.l2 = nn.Linear(64, 32)\n",
    "        self.l3 = nn.Linear(32, 1) \n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.l1(x)      # x.shape (n, 10) --> (n, 64)  \n",
    "        x = F.relu(x)  \n",
    "        x = self.l2(x)      # x.shape (n, 64) --> (n, 32)  \n",
    "        x = F.relu(x) \n",
    "        x = self.l3(x)      # x.shape (n, 32) --> (n,  1) \n",
    "        return x \n",
    "\n",
    "net = Net()\n",
    "print(net.l2.weight.shape, net.l2.bias.shape)\n",
    "# 创建输入测试ANN\n",
    "input_data = torch.randn(2, 10)  # 假设输入维度为10\n",
    "\n",
    "# 前向传播\n",
    "outputs = net(input_data)\n",
    "print(outputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89fbf0a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-29T22:46:30.003037Z",
     "start_time": "2024-03-29T22:46:29.831762Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 823), (1, 824), (2, 785), (3, 783), (4, 790), (5, 806), (6, 765), (7, 834), (8, 790), (9, 800)]\n",
      "[(0, 93), (1, 100), (2, 116), (3, 95), (4, 90), (5, 109), (6, 111), (7, 99), (8, 105), (9, 82)]\n",
      "[(0, 100), (1, 99), (2, 99), (3, 117), (4, 92), (5, 111), (6, 90), (7, 97), (8, 111), (9, 84)]\n",
      "8000 1000 1000\n",
      "testloader第0次吐出64组数据\n",
      "testloader第1次吐出64组数据\n",
      "testloader第2次吐出64组数据\n",
      "testloader第3次吐出64组数据\n",
      "testloader第4次吐出64组数据\n",
      "testloader第5次吐出64组数据\n",
      "testloader第6次吐出64组数据\n",
      "testloader第7次吐出64组数据\n",
      "testloader第8次吐出64组数据\n",
      "testloader第9次吐出64组数据\n",
      "testloader第10次吐出64组数据\n",
      "testloader第11次吐出64组数据\n",
      "testloader第12次吐出64组数据\n",
      "testloader第13次吐出64组数据\n",
      "testloader第14次吐出64组数据\n",
      "testloader第15次吐出40组数据\n",
      "testloader总共已吐出10000组数据\n",
      "最后一组吐出的40组数据是: tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]]) \n",
      " tensor([[18.],\n",
      "        [13.],\n",
      "        [16.],\n",
      "        [14.],\n",
      "        [14.],\n",
      "        [18.],\n",
      "        [15.],\n",
      "        [11.],\n",
      "        [12.],\n",
      "        [17.],\n",
      "        [14.],\n",
      "        [15.],\n",
      "        [18.],\n",
      "        [13.],\n",
      "        [16.],\n",
      "        [19.],\n",
      "        [11.],\n",
      "        [16.],\n",
      "        [18.],\n",
      "        [18.],\n",
      "        [20.],\n",
      "        [14.],\n",
      "        [16.],\n",
      "        [15.],\n",
      "        [18.],\n",
      "        [19.],\n",
      "        [18.],\n",
      "        [13.],\n",
      "        [11.],\n",
      "        [15.],\n",
      "        [18.],\n",
      "        [20.],\n",
      "        [18.],\n",
      "        [17.],\n",
      "        [11.],\n",
      "        [16.],\n",
      "        [17.],\n",
      "        [11.],\n",
      "        [14.],\n",
      "        [18.]])\n"
     ]
    }
   ],
   "source": [
    "import random, numpy as np\n",
    "random.seed(123)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torch.utils.data import TensorDataset, Subset, DataLoader\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')  \n",
    "enc.fit([[11], [12], [13], [14], [15], [16], [17], [18], [19], [20]])\n",
    "_labels    = [[random.randint(11, 20)] for _ in range(10000)]\n",
    "_features = enc.transform(_labels).toarray()\n",
    "\n",
    "sss = StratifiedShuffleSplit(test_size=0.2, random_state=123)  \n",
    "train_idx, test_idx = next(sss.split(_features, np.array(_labels).flatten())) \n",
    "sss = StratifiedShuffleSplit(test_size=0.5, random_state=123) \n",
    "valid_idx, test_idx = next(sss.split(_features[test_idx], np.array(_labels).flatten()[test_idx])) \n",
    "\n",
    "##################### test purpose ########################## \n",
    "c = _features[train_idx].argmax(axis=1) \n",
    "print([(i, len([j for j in c if j==i])) for i in range(10)])\n",
    "\n",
    "c = _features[valid_idx].argmax(axis=1) \n",
    "print([(i, len([j for j in c if j==i])) for i in range(10)])\n",
    "\n",
    "c = _features[test_idx].argmax(axis=1) \n",
    "print([(i, len([j for j in c if j==i])) for i in range(10)]) \n",
    "\n",
    "print(len(train_idx), len(valid_idx), len(test_idx)) \n",
    "#############################################################\n",
    "\n",
    "features  = torch.tensor(_features, dtype=torch.float32)\n",
    "labels     = torch.tensor(_labels   , dtype=torch.float32)\n",
    "dataset = TensorDataset(features, labels)\n",
    "\n",
    "trainingset = Subset(dataset, train_idx)\n",
    "trainloader = DataLoader(trainingset, batch_size=64, shuffle=True) \n",
    "\n",
    "validset = Subset(dataset, valid_idx)\n",
    "validloader = DataLoader(validset, batch_size=64, shuffle=True) \n",
    "\n",
    "testset = Subset(dataset, test_idx)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=True) \n",
    "\n",
    "##################### test purpose ############################ \n",
    "for i, (features, labels) in enumerate(testloader,0):\n",
    "    print(f'testloader第{i}次吐出{len(labels)}组数据')\n",
    "print(f'testloader总共已吐出{len(testset.dataset)}组数据')\n",
    "print(f'最后一组吐出的{len(labels)}组数据是:',features, '\\n', labels)\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26f48147",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-29T22:46:30.490761Z",
     "start_time": "2024-03-29T22:46:30.003799Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] Training Loss: 12.821\n",
      "[1,    21] Training Loss: 158.067\n",
      "[1,    41] Training Loss: 18.952\n",
      "[1,    61] Training Loss: 2.091\n",
      "[1,    81] Training Loss: 0.222\n",
      "[1,   101] Training Loss: 0.028\n",
      "[1,   121] Training Loss: 0.004\n",
      "[1] Validation Loss: 1.6211285372264683e-05\n",
      "Validation Loss Decreased (inf--->0.016211)\n",
      "Better Model Saved as best_model.pth\n",
      "[2,     1] Training Loss: 0.000\n",
      "[2,    21] Training Loss: 0.001\n",
      "[2,    41] Training Loss: 0.000\n",
      "[2,    61] Training Loss: 0.000\n",
      "[2,    81] Training Loss: 0.000\n",
      "[2,   101] Training Loss: 0.000\n",
      "[2,   121] Training Loss: 0.000\n",
      "[2] Validation Loss: 7.190664882728016e-08\n",
      "Validation Loss Decreased (0.016211--->0.000072)\n",
      "Better Model Saved as best_model.pth\n",
      "[3,     1] Training Loss: 0.000\n",
      "[3,    21] Training Loss: 0.000\n",
      "[3,    41] Training Loss: 0.000\n",
      "[3,    61] Training Loss: 0.000\n",
      "[3,    81] Training Loss: 0.000\n",
      "[3,   101] Training Loss: 0.000\n",
      "[3,   121] Training Loss: 0.000\n",
      "[3] Validation Loss: 3.1739672579078617e-10\n",
      "Validation Loss Decreased (0.000072--->0.000000)\n",
      "Better Model Saved as best_model.pth\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    " \n",
    "# Training with Validation\n",
    "epochs = 3\n",
    "min_valid_loss = np.inf\n",
    " \n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    net.train()\n",
    "    for i, (features, labels) in enumerate(trainloader, start=0): \n",
    "        outputs = net(features)\n",
    "        loss = criterion(outputs,labels)  \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "        train_loss += loss.item()\n",
    "        if i % 20 == 0:    # print every 20 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] Training Loss: {train_loss / 20:.3f}')\n",
    "            train_loss = 0.0\n",
    "     \n",
    "    valid_loss = 0.0\n",
    "    net.eval()     # Optional when not using Model Specific layer\n",
    "    with torch.no_grad():\n",
    "        for _, (features, labels) in enumerate(validloader, start=0):   \n",
    "            outputs = net(features) \n",
    "            loss = criterion(outputs,labels) \n",
    "            valid_loss += loss.item()\n",
    "        print(f'[{epoch + 1}] Validation Loss: {valid_loss / len(validloader.dataset)}')\n",
    "     \n",
    "    if min_valid_loss > valid_loss:\n",
    "        print(f'Validation Loss Decreased ({min_valid_loss:.6f}--->{valid_loss:.6f})')\n",
    "        min_valid_loss = valid_loss\n",
    "        torch.save(net.state_dict(), 'best_model.pth')\n",
    "        print(f'Better Model Saved as best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0344d8d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-29T22:46:30.503402Z",
     "start_time": "2024-03-29T22:46:30.492032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 1000 samples: 100 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0 \n",
    "    for _, (features, labels) in enumerate(testloader, start=0):  \n",
    "        outputs = net(features) \n",
    "        correct += sum(np.isclose(outputs,labels,atol=1e-3).flatten())\n",
    "    print(f'Accuracy of the network on the {len(testloader.dataset)}\\\n",
    " samples: {100 * correct // len(testloader.dataset)} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
