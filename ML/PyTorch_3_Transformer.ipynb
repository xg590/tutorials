{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "306afab7-87f8-4f98-b92b-7a81800836d1",
   "metadata": {},
   "source": [
    "* [1. Original Paper](https://arxiv.org/html/1706.03762v7)\n",
    "* [2. Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02744460-ac9d-4a41-b85b-f8615a2efdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, numpy as np, matplotlib.pyplot as plt, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "N, H, SEED    = 2, 8, 442\n",
    "BATCH_SIZE    = 80\n",
    "DROPOUT_RATE  = 0.1\n",
    "EMBEDDING_DIM = 512\n",
    "assert EMBEDDING_DIM % H == 0\n",
    "random      .seed(SEED)\n",
    "np   .random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a3c915-9cf6-4f23-88fc-52cf8a81d22a",
   "metadata": {},
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "901045e5-f711-4e77-a243-b95afad3f796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'] \n",
      " ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] \n",
      " 65 90 97 122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['NLOCO', 'SBIIMTD', 'BAXQUFLXZGSN'],\n",
       " ['nnllooccoo', 'ssbbiiiimmttdd', 'bbaaxxqquuffllxxzzggssnn'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB = ['<BOS>', '<EOS>', '<PAD>', \n",
    "         'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', \n",
    "         'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "\n",
    "def tokenizer(s, length_max):\n",
    "    s = [VOCAB.index('<BOS>')] + [VOCAB.index(c) for c in s] + \\\n",
    "        [VOCAB.index('<EOS>')] + [VOCAB.index('<PAD>') for i in range(length_max - len(s))]\n",
    "    return s\n",
    "\n",
    "tokenizer('AAA', len('AAA')), tokenizer('AAA', len('AAAAAA'))\n",
    "\n",
    "def text_gen(length_min, length_max, batch_size = BATCH_SIZE):\n",
    "    ''' Throw out random length of uppercase letters, faking sentences\n",
    "    Throw out corresponding sentences in lowercase letter but double its length'''\n",
    "    stn_len = np.random.randint(length_min, length_max+1, size=batch_size)\n",
    "    # 'A':65, 'Z':90, 'a': 97, 'z':122\n",
    "    upper_case = [np.random.randint(65, 91, size=i)\n",
    "                    for i in stn_len] \n",
    "    lower_case = [''.join([chr(i+32)\n",
    "                    for i in np.array([\n",
    "                        (j, j) for j in k]).flatten()]) # HERE\n",
    "                                  for k in upper_case]\n",
    "    upper_case = [''.join([chr(i) for i in j]) for j in upper_case]\n",
    "    return upper_case, lower_case\n",
    "\n",
    "print('', [chr(i) for i in range(65, 91)], '\\n', [chr(i) for i in range(97, 123)], '\\n', ord('A'), ord('Z'), ord('a'), ord('z'))\n",
    "text_gen(3, 13, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e061cfee-13fa-415c-9b3c-53ff165be3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src\n",
      "<BOS>\tR\tK\tS\tW\tT\t<EOS>\t<PAD>\t<PAD>\n",
      "<BOS>\tO\tB\tX\tH\tH\tE\t<EOS>\t<PAD>\n",
      "tgt_in\n",
      "<BOS>\tr\tr\tk\tk\ts\ts\tw\tw\tt\tt\t<EOS>\t<PAD>\t<PAD>\t<PAD>\n",
      "<BOS>\to\to\tb\tb\tx\tx\th\th\th\th\te\te\t<EOS>\t<PAD>\n",
      "tgt_out\n",
      "r\tr\tk\tk\ts\ts\tw\tw\tt\tt\t<EOS>\t<PAD>\t<PAD>\t<PAD>\t<PAD>\n",
      "o\to\tb\tb\tx\tx\th\th\th\th\te\te\t<EOS>\t<PAD>\t<PAD>\n"
     ]
    }
   ],
   "source": [
    "class Batch:\n",
    "    def __init__(self, UPPER, lower = None):\n",
    "        self.UPPER = UPPER\n",
    "        UPPER_length_max = max([len(s) for s in UPPER])\n",
    "        self.src = torch.LongTensor([tokenizer(s, UPPER_length_max) for s in UPPER])\n",
    "\n",
    "        if lower:\n",
    "            self.lower = lower\n",
    "            lower_length_max = max([len(s) for s in lower])\n",
    "            self.tgt = torch.LongTensor([tokenizer(s, lower_length_max) for s in lower])\n",
    "            self.tgt_in, self.tgt_out = self.tgt[:, :-1], self.tgt[:, 1:]\n",
    "            self.tgt_mask = self.mask(lower_length_max+1)\n",
    "            self.ntokens = (self.tgt_out != VOCAB.index('<PAD>')).data.sum() # count tokens\n",
    "\n",
    "        # src, tgt, tgt_in, tgt_out, tgt_mask, ntokens\n",
    "\n",
    "    def mask(self, size):\n",
    "        return torch.triu(torch.ones(size, size, dtype = int), diagonal = 1) == 1\n",
    "\n",
    "def data_gen(length_min = 10, length_max = 15, nbatch=10):\n",
    "    for i in range(nbatch):\n",
    "        UPPER, lower = text_gen(length_min, length_max)\n",
    "        yield Batch(UPPER, lower)\n",
    "\n",
    "for i, batch in enumerate(data_gen(3, 7, 2)):\n",
    "    break\n",
    "    print(batch.UPPER, batch.lower)\n",
    "    print(batch.src.shape)\n",
    "    print([[VOCAB[i] for i in j] for j in batch.src][0])\n",
    "    print(batch.tgt.shape)\n",
    "    print([[VOCAB[i] for i in j] for j in batch.tgt][0])\n",
    "    print(batch.tgt_in.shape)\n",
    "    print([[VOCAB[i] for i in j] for j in batch.tgt_in][0])\n",
    "    print(batch.tgt_out.shape)\n",
    "    print('        ', [[VOCAB[i] for i in j] for j in batch.tgt_out][0])\n",
    "    print(batch.tgt_mask.shape)\n",
    "    print('---ntokens---', batch.ntokens)\n",
    "\n",
    "def decipher(tnsr):\n",
    "    [print('\\t'.join([VOCAB[i] for idx, i in enumerate(j)])) for j in tnsr]\n",
    "\n",
    "print('src')\n",
    "decipher(batch.src[:2])\n",
    "print('tgt_in')\n",
    "decipher(batch.tgt_in[:2])\n",
    "print('tgt_out')\n",
    "decipher(batch.tgt_out[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f8125e-ee09-4cae-85c9-36f316d673f1",
   "metadata": {},
   "source": [
    "### Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a1f1be0-068a-48bf-8b5a-43fcc2a79295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([80, 9, 512]), torch.Size([80, 15, 512]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x * np.sqrt(EMBEDDING_DIM)\n",
    "        return x\n",
    "\n",
    "embd = Embedder()\n",
    "embd.forward(batch.src).shape, embd.forward(batch.tgt_in).shape # (8-2) * 3 + 2 = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0356478f-f818-4fe8-b0ee-1d4e741c8fc5",
   "metadata": {},
   "source": [
    "$$PE_{(pos,2i)} = \\sin(pos / 10000^{2i/d_{\\text{model}}})$$\n",
    "$$PE_{(pos,2i+1)} = \\cos(pos / 10000^{2i/d_{\\text{model}}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd5e6ede-17e0-462c-8b99-b6acc2ece871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 9, 512])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, max_len=5000):\n",
    "        super().__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, EMBEDDING_DIM)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(-np.log(10000.0) * torch.arange(0, EMBEDDING_DIM, 2) / EMBEDDING_DIM)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        x = F.dropout(x, p = DROPOUT_RATE)\n",
    "        return x\n",
    "\n",
    "posEnc = PositionalEncoder()\n",
    "posEnc.forward(embd.forward(batch.src)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b56cab1-9dc2-4671-bd92-78003b780ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 9, 512])\n",
      "torch.Size([80, 15, 512])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Q_linear = nn.Linear(EMBEDDING_DIM, EMBEDDING_DIM)\n",
    "        self.V_linear = nn.Linear(EMBEDDING_DIM, EMBEDDING_DIM)\n",
    "        self.K_linear = nn.Linear(EMBEDDING_DIM, EMBEDDING_DIM)\n",
    "        self.out      = nn.Linear(EMBEDDING_DIM, EMBEDDING_DIM)\n",
    "\n",
    "    def forward(self, Q, K, V, mask = None): #\n",
    "        bs = Q.size(0)\n",
    "        sl = Q.size(1) # sequence_length\n",
    "        dk = EMBEDDING_DIM // H # embed_dim / head_count\n",
    "        Q  = self.Q_linear(Q).view(bs, sl, H, dk).transpose(1, 2)\n",
    "        sl = K.size(1) # sequence_length\n",
    "        KT = self.K_linear(K).view(bs, sl, H, dk).transpose(1, 2).transpose(2, 3)\n",
    "        V  = self.V_linear(V).view(bs, sl, H, dk).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, KT) / np.sqrt(dk)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask, -1e9)\n",
    "        scores = scores.softmax(dim = -1)\n",
    "        scores = F.dropout(scores, p = DROPOUT_RATE)\n",
    "\n",
    "        scores = torch.matmul(scores, V)\n",
    "        scores = scores.transpose(1,2).contiguous().view(bs, -1, EMBEDDING_DIM) # Concatenation\n",
    "        scores = self.out(scores)\n",
    "        scores = F.dropout(scores, p = DROPOUT_RATE)\n",
    "\n",
    "        return scores\n",
    "\n",
    "mha = MultiHeadAttention()\n",
    "x = posEnc.forward(embd.forward(batch.src))\n",
    "m = mha(Q = x, K = x, V = x)\n",
    "print(m.shape)\n",
    "x = posEnc.forward(embd.forward(batch.tgt_in))\n",
    "print(mha(Q = x, K = m, V = m).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5d5419-32cf-4251-8b5b-0ffe11697abb",
   "metadata": {},
   "source": [
    "$$\\mathrm{LN}(x)=  \\frac{x-\\mu}{\\sigma} \\cdot \\gamma+\\beta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b2bd1c3-dbff-41a2-9e4c-1fc7070be0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norm(nn.Module):\n",
    "    def __init__(self, eps = 1e-6):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones (EMBEDDING_DIM)) # weight\n",
    "        self.beta  = nn.Parameter(torch.zeros(EMBEDDING_DIM)) # bias\n",
    "        self.eps   = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim = True)\n",
    "        std  = x.std (-1, keepdim = True) # standard deviation\n",
    "        x    = (x - mean) / (std + self.eps) * self.gamma + self.beta\n",
    "        return x\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, feedforward_dim=2048, dropout_rate = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(EMBEDDING_DIM, feedforward_dim)\n",
    "        self.linear_2 = nn.Linear(feedforward_dim, EMBEDDING_DIM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=DROPOUT_RATE)\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff07743e-7325-488d-ae82-eacaf4fb6a60",
   "metadata": {},
   "source": [
    "### Encoder and Decoder\n",
    "see eq. 2: $ y_l = x_l+Module(Norm(x_l)) \\ $ in [this paper](https://arxiv.org/pdf/2502.02732v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fbce50d-4057-4d63-a111-7d188a9ea206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 9, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mhsa   = MultiHeadAttention()\n",
    "        self.norm_1 = Norm()\n",
    "        self.ff     = FeedForward()\n",
    "        self.norm_2 = Norm()\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''see eq. 2 in https://arxiv.org/pdf/2502.02732v1 for \n",
    "        Rre-LN (Norm before addition to the residue)'''\n",
    "        # Norm\n",
    "        y = self.norm_1(x)\n",
    "        y = F.dropout(y, p=DROPOUT_RATE)\n",
    "        # Add\n",
    "        x = x + self.mhsa(y, y, y)\n",
    "        # Norm\n",
    "        y = self.norm_2(x)\n",
    "        y = F.dropout(y, p=DROPOUT_RATE)\n",
    "        # Add\n",
    "        x = x + self.ff(y)\n",
    "        return x\n",
    "\n",
    "enc = Encoder()\n",
    "x = posEnc.forward(embd.forward(batch.src))\n",
    "m = enc.forward(x)\n",
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fc76382-09a1-483e-9450-c7e2ef87d335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([80, 15, 512]),\n",
       " torch.Size([80, 9, 512]),\n",
       " torch.Size([80, 15, 512]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mmhsa  = MultiHeadAttention()\n",
    "        self.norm_1 = Norm()\n",
    "        self.mhca   = MultiHeadAttention()\n",
    "        self.norm_2 = Norm()\n",
    "        self.ff     = FeedForward()\n",
    "        self.norm_3 = Norm()\n",
    "\n",
    "    def forward(self, mem, tgt, mask):\n",
    "        # Norm\n",
    "        y = self.norm_1(tgt)\n",
    "        y = F.dropout(y, p=DROPOUT_RATE)\n",
    "        # Add\n",
    "        tgt = tgt + self.mmhsa(y, y, y, mask)\n",
    "        # Norm\n",
    "        y = self.norm_2(tgt)\n",
    "        y = F.dropout(y, p=DROPOUT_RATE)\n",
    "        # Add\n",
    "        tgt = tgt + self.mhca(y, mem, mem) # (Q, K, V, mask)\n",
    "        # Norm\n",
    "        y = self.norm_3(tgt)\n",
    "        # Add\n",
    "        tgt = tgt + self.ff(y)\n",
    "        return tgt\n",
    "\n",
    "dec = Decoder()\n",
    "x = posEnc.forward(embd.forward(batch.tgt_in))\n",
    "dec.forward(m, x, batch.tgt_mask).shape, m.shape, x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1bde91-6380-4f09-a2c3-5bd7b32560aa",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17eb716f-3d15-4fad-a270-8b8d7860ac13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src torch.Size([80, 9])\n",
      "mem torch.Size([1, 9, 512])\n",
      "['0<BOS> 1B']\n",
      "['0<BOS> 1W 2h']\n",
      "['0<BOS> 1t 2W 3h']\n",
      "['0<BOS> 1t 2<EOS> 3W 4D']\n",
      "['0<BOS> 1<EOS> 2W 3B 4W 5<EOS>']\n",
      "['0<BOS> 1J 2X 3W 4W 5W 6X']\n",
      "['0<BOS> 1<EOS> 2B 3g 4W 5W 6W 7<EOS>']\n",
      "['0<BOS> 1J 2<EOS> 3W 4V 5W 6W 7W 8W']\n",
      "['0<BOS> 1J 2<EOS> 3E 4D 5<EOS> 6W 7D 8D 9W']\n",
      "['0<BOS> 1t 2B 3D 4W 5a 6<EOS> 7W 8O 9a 10g']\n"
     ]
    }
   ],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.src_embd = Embedder()\n",
    "        self.tgt_embd = Embedder()\n",
    "        self.posEnc   = PositionalEncoder()\n",
    "        self.nEnc     = nn.ModuleList([Encoder() for i in range(N)])\n",
    "        self.nDec     = nn.ModuleList([Decoder() for i in range(N)])\n",
    "        self.proj     = nn.Linear(EMBEDDING_DIM, VOCAB_SIZE)\n",
    "\n",
    "        [nn.init.xavier_uniform_(p) for p in self.parameters() if p.dim() > 1]\n",
    "\n",
    "    def encode(self, src):\n",
    "        mem = self.src_embd(src)\n",
    "        mem = self.posEnc(mem)\n",
    "        for enc in self.nEnc:\n",
    "            mem = enc(mem)\n",
    "        return mem\n",
    "\n",
    "    def decode(self, mem, tgt, tgt_mask):\n",
    "        tgt = self.tgt_embd(tgt)\n",
    "        tgt = self.posEnc(tgt)\n",
    "        for dec in self.nDec:\n",
    "            tgt = dec(mem, tgt, tgt_mask)\n",
    "        return tgt\n",
    "\n",
    "    def gen(self, tgt):\n",
    "        tgt = self.proj(tgt)\n",
    "        tgt = F.log_softmax(tgt, dim=-1)\n",
    "        return tgt\n",
    "\n",
    "model = Transformer()\n",
    "\n",
    "if 1:\n",
    "    src_ = batch.src[:1, :]\n",
    "    mem = model.encode(src_)\n",
    "    print('src', batch.src.shape)\n",
    "    print('mem', mem.shape)\n",
    "    BOS = torch.empty(1, 1, dtype=int).fill_(VOCAB.index('<BOS>'))\n",
    "    pred = BOS\n",
    "    for i in range(10):\n",
    "        pred = model.decode(mem, pred, None)\n",
    "        pred = model.gen(pred)\n",
    "        _, pred = torch.max(pred, dim = -1)\n",
    "        pred = torch.cat([BOS, pred], dim = -1)\n",
    "        print([' '.join([f'{idx}{VOCAB[i]}' for idx, i in enumerate(j)]) for j in pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c415cbce-09e0-43f4-8119-f7daab523093",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2ae3aed-d15e-4290-9cfd-a64eb9a223ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.1024, grad_fn=<AddBackward0>)\n",
      "tensor(3.8499, grad_fn=<AddBackward0>)\n",
      "tensor(3.2130, grad_fn=<AddBackward0>)\n",
      "tensor(2.8292, grad_fn=<AddBackward0>)\n",
      "tensor(2.5104, grad_fn=<AddBackward0>)\n",
      "tensor(2.2470, grad_fn=<AddBackward0>)\n",
      "tensor(1.9842, grad_fn=<AddBackward0>)\n",
      "tensor(1.8702, grad_fn=<AddBackward0>)\n",
      "tensor(1.6249, grad_fn=<AddBackward0>)\n",
      "tensor(1.5020, grad_fn=<AddBackward0>)\n",
      "tensor(1.4005, grad_fn=<AddBackward0>)\n",
      "tensor(1.3334, grad_fn=<AddBackward0>)\n",
      "tensor(1.2849, grad_fn=<AddBackward0>)\n",
      "tensor(1.2416, grad_fn=<AddBackward0>)\n",
      "tensor(1.1860, grad_fn=<AddBackward0>)\n",
      "tensor(1.0869, grad_fn=<AddBackward0>)\n",
      "tensor(1.1262, grad_fn=<AddBackward0>)\n",
      "tensor(1.0565, grad_fn=<AddBackward0>)\n",
      "tensor(1.0188, grad_fn=<AddBackward0>)\n",
      "tensor(1.0290, grad_fn=<AddBackward0>)\n",
      "tensor(0.9996, grad_fn=<AddBackward0>)\n",
      "tensor(0.9539, grad_fn=<AddBackward0>)\n",
      "tensor(0.9362, grad_fn=<AddBackward0>)\n",
      "tensor(0.9101, grad_fn=<AddBackward0>)\n",
      "tensor(0.8885, grad_fn=<AddBackward0>)\n",
      "tensor(0.8805, grad_fn=<AddBackward0>)\n",
      "tensor(0.8408, grad_fn=<AddBackward0>)\n",
      "tensor(0.8443, grad_fn=<AddBackward0>)\n",
      "tensor(0.8243, grad_fn=<AddBackward0>)\n",
      "tensor(0.8266, grad_fn=<AddBackward0>)\n",
      "tensor(0.8193, grad_fn=<AddBackward0>)\n",
      "tensor(0.7840, grad_fn=<AddBackward0>)\n",
      "tensor(0.7765, grad_fn=<AddBackward0>)\n",
      "tensor(0.7813, grad_fn=<AddBackward0>)\n",
      "tensor(0.7751, grad_fn=<AddBackward0>)\n",
      "tensor(0.7702, grad_fn=<AddBackward0>)\n",
      "tensor(0.7654, grad_fn=<AddBackward0>)\n",
      "tensor(0.7596, grad_fn=<AddBackward0>)\n",
      "tensor(0.7685, grad_fn=<AddBackward0>)\n",
      "tensor(0.7627, grad_fn=<AddBackward0>)\n",
      "tensor(0.7555, grad_fn=<AddBackward0>)\n",
      "tensor(0.7472, grad_fn=<AddBackward0>)\n",
      "tensor(0.7569, grad_fn=<AddBackward0>)\n",
      "tensor(0.7480, grad_fn=<AddBackward0>)\n",
      "tensor(0.7441, grad_fn=<AddBackward0>)\n",
      "tensor(0.7467, grad_fn=<AddBackward0>)\n",
      "tensor(0.7459, grad_fn=<AddBackward0>)\n",
      "tensor(0.7402, grad_fn=<AddBackward0>)\n",
      "tensor(0.7436, grad_fn=<AddBackward0>)\n",
      "tensor(0.7398, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = Transformer()\n",
    "model.train() # train mode\n",
    "loss_func    = nn.CrossEntropyLoss(ignore_index = VOCAB.index('<PAD>'), label_smoothing = 0.1)\n",
    "optimizer    = torch.optim.Adam(model.parameters(), lr = 0.5, betas = (0.9, 0.98), eps = 1e-9)\n",
    "rate         = lambda step: 0.0442 * min(step ** (-0.5), step * 0.000125) if step else 5e-06\n",
    "lr_scheduler = LambdaLR( optimizer = optimizer, lr_lambda = rate)\n",
    "\n",
    "epoch = 100\n",
    "for _ in range(epoch):\n",
    "    for i, batch in enumerate(data_gen(length_min = 10, length_max = 15, nbatch = 20)):\n",
    "        mem = model.encode(batch.src)\n",
    "        tgt_out_ = model.decode(mem, batch.tgt_in, batch.tgt_mask)\n",
    "        tgt_out_ = model.gen(tgt_out_)\n",
    "        loss = loss_func(tgt_out_.view(-1, tgt_out_.size(-1)), batch.tgt_out.reshape(-1)) \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "    print(loss)\n",
    "    if loss < 0.74: break "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99749c88-1137-4005-9c4b-b208d5fcac78",
   "metadata": {},
   "source": [
    "### Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d1168f4-28a9-40af-9692-ae903369a62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS>\tC\tX\tN\tG\tQ\t<EOS>\n",
      "c\tc\tx\tx\tn\tn\tg\tg\tq\tq\t<EOS>\tx\tn\tn\tg\tx\tg\tx\tx\tq\tg\tq\tn\tq\t<EOS>\tc\tn\tc\tx\tg\n",
      "<BOS>\tE\tM\tB\tE\tD\tD\tI\tN\tG\tD\tI\tM\t<EOS>\n",
      "i\te\tm\tm\tb\tb\te\te\td\td\td\td\ti\ti\tn\tn\tg\td\td\td\ti\ti\tm\tm\t<EOS>\td\td\td\th\tt\n"
     ]
    }
   ],
   "source": [
    "model.eval() # train mode\n",
    "def predict(s):\n",
    "    b = Batch([s])\n",
    "    decipher(b.src)\n",
    "    mem = model.encode(b.src)\n",
    "    BOS = torch.empty(1, 1, dtype=int).fill_(VOCAB.index('<BOS>'))\n",
    "    ys = BOS\n",
    "    for i in range(30):\n",
    "        pred = model.decode(mem, ys, None)\n",
    "        pred = model.gen(pred)\n",
    "        _, pred = torch.max(pred, dim = -1)\n",
    "        ys = torch.cat([BOS, pred], dim = -1)\n",
    "    [print('\\t'.join([VOCAB[i] for idx, i in enumerate(j)])) for j in pred]\n",
    "\n",
    "predict('CXNGQ')\n",
    "predict('EMBEDDINGDIM')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
