{
 "cells": [
  {
   "cell_type": "raw",
   "id": "66b55ed5",
   "metadata": {},
   "source": [
    "!pip install datasets evaluate transformers scikit-learn scipy accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d8f7e0",
   "metadata": {},
   "source": [
    "### This notebook is based on the [course](https://huggingface.co/course/en/chapter7/6?fw=pt) from hugging face.\n",
    "* This job was trained on NYU HPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f52cbcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch\n",
    "os.environ[\"HF_HOME\"] = f'/scratch/{os.environ[\"USER\"]}/huggingface'\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, load_dataset, DatasetDict\n",
    "from accelerate import Accelerator \n",
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig, \\\n",
    "                         get_scheduler, DataCollatorForLanguageModeling\n",
    "from torch.nn import CrossEntropyLoss \n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data.dataloader import DataLoader \n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "if 0 and torch.cuda.device_count():\n",
    "    print (torch.cuda.device_count(), 'CUDA device(s) count be found on this system')\n",
    "elif 0:\n",
    "    print('No CUDA devices on this system')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d55397b",
   "metadata": {},
   "source": [
    "sacctmgr list qos format=maxwall,maxtresperuser%40,name\n",
    "seff 31516423\n",
    "srun --job-name=${HOSTNAME}-${PWD##*/} --nodes=1 --time=1:00:00 --cpus-per-task=2 --mem=40GB --gres=gpu:rtx8000:2 --pty /bin/bash\n",
    "sinfo -o \"%20N  %10c  %10m  %25f  %10G \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade1909d",
   "metadata": {},
   "source": [
    "### Build new tokenizer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3bbee6f7",
   "metadata": {},
   "source": [
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\") \n",
    "\n",
    "def get_training_corpus():\n",
    "    batch_size = 1000\n",
    "    raw_datasets = load_dataset(\"code_search_net\", \"python\")\n",
    "    for i in range(0, len(raw_datasets[\"train\"]), batch_size):\n",
    "        yield raw_datasets[\"train\"][i : i + batch_size][\"whole_func_string\"] \n",
    "\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(get_training_corpus(), 52000) \n",
    "tokenizer.save_pretrained(\"new_tokenizer_dir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1856f3d",
   "metadata": {},
   "source": [
    "### Build new Python Code dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b554f25",
   "metadata": {},
   "source": [
    "split = \"train\"  # \"valid\"\n",
    "dataset = load_dataset(f\"transformersbook/codeparrot-{split}\", split=split, streaming=True) \n",
    "\n",
    "filtered_dict = defaultdict(list)\n",
    "total = 0\n",
    "for sample in tqdm(iter(dataset)):\n",
    "    total += 1\n",
    "    if total > 500_000: break\n",
    "    for keyword in [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]:\n",
    "        if keyword in sample[\"content\"]: \n",
    "            for k, v in sample.items():\n",
    "                filtered_dict[k].append(v)\n",
    "            break\n",
    "print(f\"{len(filtered_dict['content'])/total:.2%} of data after filtering.\")\n",
    "raw_datasets = Dataset.from_dict(filtered_dict)\n",
    "\n",
    "print(raw_datasets[0]['content'][:1000]) \n",
    "raw_datasets.save_to_disk('new_dataset_dir')\n",
    "raw_datasets = Dataset.load_from_disk('new_dataset_dir')\n",
    "print(raw_datasets[1]['content'][:100]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf64afc",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eeb31fe3",
   "metadata": {},
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\n",
    "ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\n",
    "\n",
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": ds_train,  # .shuffle().select(range(50000)),\n",
    "        \"valid\": ds_valid,  # .shuffle().select(range(500))\n",
    "    }\n",
    ")\n",
    "\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0f22ebd6",
   "metadata": {},
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"content\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,      # Each return output is 128-maximum-length\n",
    "        return_overflowing_tokens=True, # With this argment, one element was chopped into several chunks. \n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize, \n",
    "    batched=True,\n",
    "    num_proc = int(os.environ['SLURM_CPUS_PER_TASK']),\n",
    "    remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets # executed in 3m 3s, finished 14:35:15 2023-03-31\n",
    "tokenized_datasets.save_to_disk('/scratch/xg590/new_tokenized_datasets_dir')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2482ffa4",
   "metadata": {},
   "source": [
    "### Training with ðŸ¤— Accelerate\n",
    "```\n",
    "from accelerate import Accelerator \n",
    "\n",
    "accelerator = Accelerator()\n",
    "train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n",
    "\ttrain_dataloader, eval_dataloader, model, optimizer\n",
    ") \n",
    "accelerator.backward(loss)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "259a5d60",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def training_function():  \n",
    "    accelerator = Accelerator() \n",
    "    hyperparameters = {\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"num_epochs\": 3,\n",
    "        \"train_batch_size\": 8, # Actual batch size will this x 8\n",
    "        \"eval_batch_size\": 32, # Actual batch size will this x 8\n",
    "        \"seed\": 42,\n",
    "    }\n",
    "    set_seed(hyperparameters[\"seed\"])\n",
    "        \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"new_tokenizer_dir\")   \n",
    "    tokenizer.pad_token = tokenizer.eos_token  \n",
    "\n",
    "    context_length = 128 \n",
    "    config = AutoConfig.from_pretrained(\n",
    "        \"gpt2\",\n",
    "        vocab_size=len(tokenizer),\n",
    "        n_ctx=context_length,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    model = GPT2LMHeadModel(config)\n",
    "    #print(tokenizer.vocab_size, model.lm_head.out_features) \n",
    "    accelerator.print(f\"GPT-2 size: {sum(t.numel() for t in model.parameters())/1e6:.1f}M parameters\")\n",
    "    def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "        weight_decay = 0.1 \n",
    "        params_with_wd, params_without_wd = [], []\n",
    "        for n, p in model.named_parameters():\n",
    "            if any(nd in n for nd in no_decay):\n",
    "                params_without_wd.append(p)\n",
    "            else:\n",
    "                params_with_wd.append(p)\n",
    "        return [\n",
    "            {\"params\": params_with_wd, \"weight_decay\": weight_decay},\n",
    "            {\"params\": params_without_wd, \"weight_decay\": 0.0},\n",
    "        ]\n",
    "    \n",
    "    optimizer = AdamW(get_grouped_params(model), lr=5e-4)\n",
    "    \n",
    "    tokenized_datasets = DatasetDict.load_from_disk(f'/scratch/{os.environ[\"USER\"]}/new_tokenized_datasets_dir')\n",
    "    # Dataset.from_dict(tokenized_datasets['train'][:100]) \n",
    "    tokenized_datasets.set_format(\"torch\")\n",
    "    train_dataloader = DataLoader(tokenized_datasets[\"train\"], batch_size=32, shuffle=True)\n",
    "    eval_dataloader  = DataLoader(tokenized_datasets[\"valid\"], batch_size=32)\n",
    "     \n",
    "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader\n",
    "    )\n",
    "    \n",
    "    num_train_epoch = 3\n",
    "    num_step_per_epoch = len(train_dataloader)\n",
    "    num_training_steps = num_train_epoch * num_step_per_epoch\n",
    "    \n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=1_000,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "    \n",
    "    keytoken_ids = []\n",
    "    for keyword in ['plt','pd','sk','fit','predict','plt',\n",
    "                    'pd','sk','fit','predict','testtest']:\n",
    "        ids = tokenizer([keyword]).input_ids[0]\n",
    "        if len(ids) == 1:\n",
    "            keytoken_ids.append(ids[0])\n",
    "        else:\n",
    "            print(f\"Keyword has not single token: {keyword}\")\n",
    "        \n",
    "    def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):\n",
    "        # Shift so that tokens < n predict n\n",
    "        shift_labels = inputs[..., 1:].contiguous()\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        # Calculate per-token loss\n",
    "        loss_fct = CrossEntropyLoss(reduce=False)\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        # Resize and average loss per sample\n",
    "        loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)\n",
    "        # Calculate and scale weighting\n",
    "        weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(\n",
    "            axis=[0, 2]\n",
    "        )\n",
    "        weights = alpha * (1.0 + weights)\n",
    "        # Calculate weighted average\n",
    "        weighted_loss = (loss_per_sample * weights).mean()\n",
    "        return weighted_loss\n",
    "    \n",
    "    output_dir = f'/scratch/{os.environ[\"USER\"]}/new_model_dir'\n",
    "    eval_steps = 5_000\n",
    "    \n",
    "    progress_bar = tqdm(range(num_training_steps), disable=not accelerator.is_local_main_process)\n",
    "    for epoch in range(num_train_epoch):\n",
    "        \n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader, start=0):\n",
    "            output = model(batch[\"input_ids\"]) \n",
    "            loss   = keytoken_weighted_loss(batch[\"input_ids\"], output.logits, keytoken_ids) \n",
    "            optimizer.zero_grad()\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            progress_bar.update(1)  \n",
    "            \n",
    "        model.eval()\n",
    "        losses = []\n",
    "        for step, batch in enumerate(eval_dataloader, start=0):\n",
    "            with torch.no_grad():\n",
    "                outputs = model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "            losses.append(accelerator.gather(outputs.loss))\n",
    "        loss = torch.mean(torch.cat(losses)) \n",
    "        eval_loss = loss.item()  \n",
    "        accelerator.print({\"loss/eval\": eval_loss})\n",
    "        # It seem I don't save model in the main process. \n",
    "        # See https://github.com/huggingface/accelerate/issues/325 for more discussion.\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc246a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 3 GPUs.\n"
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(training_function, num_processes=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
