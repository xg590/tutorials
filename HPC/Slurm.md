### Slurm Installation [Credit](https://southgreenplatform.github.io/trainings/hpc/slurminstallation/)
* Compute Node 
  * Configure the network
  * install slurm (job scheduler) and munge (inter-node uid authentication)
  * Configure slurm (create .conf files)
    ```
    echo c1 | sudo tee /etc/hostname 
    cat << EOF | sudo tee -a /etc/hosts
    127.0.0.1 c1 
    192.168.56.112 c1 
    192.168.56.111 login
    EOF
    
    sudo reboot
    
    sudo apt update  -y && \
    sudo apt install -y slurmd munge slurm-client 
    sudo mkdir             /var/spool/slurmd && \
    sudo chown slurm:slurm /var/spool/slurmd  
    
    # Create slurm.conf and cgroup.conf 
    ```
* Login Node
  *  Configure the network
  * install slurm (job scheduler) and munge (inter-node uid authentication)
  * Share the munge.key of login node with all other nodes (md5sums of /etc/munge/munge.key are the same across the cluster)
  * Configure slurm (create .conf files)
  * Restart slurm and munge across the cluster
    ```
    echo login | sudo tee /etc/hostname
    cat << EOF | sudo tee -a /etc/hosts
    127.0.0.1      login 
    192.168.56.111 login
    192.168.56.112 c1 
    EOF
    
    sudo reboot
     
    sudo apt update  -y && \
    sudo apt install -y munge slurmd slurmctld 
    
    sudo mkdir             /var/spool/slurmctld /var/spool/slurmd 
    sudo chown slurm:slurm /var/spool/slurmctld /var/spool/slurmd 
    sudo chmod 755         /var/spool/slurmctld /var/spool/slurmd 
    sudo touch /var/log/slurm/slurmctld.log /var/log/slurm/slurm_jobacct.log /var/log/    slurm/slurm_jobcomp.log 
    
    sudo scp /etc/munge/munge.key a@c1:
    ssh a@c1 "cat munge.key | sudo tee /etc/munge/munge.key 1>/dev/null 2>&1"
    
    # Create slurm.conf and cgroup.conf 
    
    sudo systemctl restart slurmctld slurmd munge && \
    sudo systemctl status  slurmctld slurmd munge 
    
    ssh a@c1 "sudo systemctl restart slurmd munge" && \
    ssh a@c1 "sudo systemctl status  slurmd munge"  
    
    munge -n | unmunge
    munge -n | ssh a@c1 unmunge
    
    ```
* Test
  ```
  srun --nodelist=login --pty /bin/bash
  srun --nodelist=c1    --pty /bin/bash 
  ```
* slurm.conf [(tutorial)](https://slurm.schedmd.com/quickstart_admin.html)
    ```
    cat << EOF | sudo tee /etc/slurm/slurm.conf
    # slurm.conf file can be generated by https://slurm.schedmd.com/configurator.html
    # also see https://github.com/SchedMD/slurm/blob/master/etc/slurm.conf.example
    AuthType=auth/munge
    ClusterName=perfect
    SlurmctldHost=login
    MpiDefault=none
    ReturnToService=1
    SlurmctldPidFile=/var/run/slurmctld.pid
    SlurmctldPort=6817
    SlurmdPidFile=/var/run/slurmd.pid
    SlurmdPort=6818
    SlurmdSpoolDir=/var/spool/slurmd
    SlurmUser=slurm
    StateSaveLocation=/var/spool/slurmctld
    SwitchType=switch/none
    ProctrackType=proctrack/cgroup
    TaskPlugin=task/affinity
    
    # TIMERS
    InactiveLimit=0
    KillWait=30
    MinJobAge=300
    SlurmctldTimeout=120
    SlurmdTimeout=300
    Waittime=0
    
    # SCHEDULING
    SchedulerType=sched/backfill
    SelectType=select/cons_tres
    
    # LOGGING AND ACCOUNTING
    AccountingStorageType=accounting_storage/none
    JobCompType=jobcomp/none
    JobAcctGatherType=jobacct_gather/none
    JobAcctGatherFrequency=30
    SlurmctldDebug=info
    SlurmctldLogFile=/var/log/slurmctld.log
    SlurmdDebug=info
    SlurmdLogFile=/var/log/slurmd.log
    
    # Node Conf
    NodeName=c1    CPUs=1 State=UNKNOWN
    NodeName=login CPUs=1 State=UNKNOWN
    
    # Partition Conf
    PartitionName=cpu Nodes=login,c1 Default=YES MaxTime=INFINITE State=UP
    
    EOF
    ```
* cgroup.conf [(manpages)](https://manpages.ubuntu.com/manpages/bionic/man5/cgroup.conf.5.html)
  ```
  cat << EOF | sudo tee /etc/slurm/cgroup.conf 
  CgroupAutomount=yes
  CgroupMountpoint=/sys/fs/cgroup
  ConstrainCores=no
  ConstrainRAMSpace=no
  EOF
  ```